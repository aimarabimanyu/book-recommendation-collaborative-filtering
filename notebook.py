# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zTHNYJ5_LAqLXBltKx4x5Zvbz6ACk96e

# Importing Libraries
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

"""# Load Dataset"""

from google.colab import drive
drive.mount('/content/drive')

books = pd.read_csv('/content/drive/MyDrive/Dataset/Rekomendasi_Buku/Books.csv')
ratings = pd.read_csv('/content/drive/MyDrive/Dataset/Rekomendasi_Buku/Ratings.csv')
users = pd.read_csv('/content/drive/MyDrive/Dataset/Rekomendasi_Buku/Users.csv')

"""# Exploratory Data Analysis

### Cek Jumlah Data Unik Pada Tiap Dataset
"""

print('Jumlah Data Unik Pada Dataset Books:')
print(books.nunique(), '\n')
print('Jumlah Data Unik Pada Dataset Ratings:')
print(ratings.nunique(), '\n')
print('Jumlah Data Unik Pada Dataset Users:')
print(users.nunique())

"""#### Cek Deskripsi Statistik Data Books Menggunakan Fungsi Info()"""

books.info()

"""#### Lihat Data Books"""

books

"""#### Cek Deskripsi Statistik Data Users Menggunakan Fungsi Info()"""

users.info()

"""#### Lihat Data Users"""

users

"""#### Cek Deskripsi Statistik Data Ratings Menggunakan Fungsi Info()"""

ratings.info()

"""#### Lihat Data Ratings"""

ratings

"""#### Cek Nilai Maksimum Pada Kolom Book-Rating"""

ratings['Book-Rating'].max()

"""#### Cek Nilai Minimum Pada Kolom Book-Rating"""

ratings['Book-Rating'].min()

"""# Data Preparation

#### Copy Dataframe Ratings ke Variabel df
"""

df = ratings

"""#### Ubah Nilai Pada Book-Rating Menjadi Float Pada Dataframe df"""

df['Book-Rating'] = df['Book-Rating'].values.astype(float)

"""#### Ubah User-ID di Dataframe df Menjadi List Tanpa Nilai Yang Sama"""

users_id = df['User-ID'].unique().tolist()

"""#### Lakukan Encoding Pada User-ID"""

users_to_encoded = {x: i for i, x in enumerate(users_id)}

"""#### Encoding Angka ke User-ID"""

encoded_to_users = {i: x for i, x in enumerate(users_id)}

"""#### Ubah ISBN Pada Dataframe df Menjadi List Tanpa Nilai Yang Sama"""

isbn = df['ISBN'].unique().tolist()

"""#### Lakukan Encoding Pada ISBN"""

isbn_to_encoded = {x: i for i, x in enumerate(isbn)}

"""#### Encoding Angka ke ISBN"""

encoded_to_isbn = {i: x for i, x in enumerate(isbn)}

"""#### Mapping User-ID dan ISBN ke Dataframe df"""

df['user'] = df['User-ID'].map(users_to_encoded)
df['book'] = df['ISBN'].map(isbn_to_encoded)

"""#### Membagi Dataframe df Menjadi Data Train dan Data Test Untuk Proses Training dan Validasi

"""

df = df.sample(frac=1, random_state=420)
df

"""#### Mapping Data User dan Book Menjadi Satu Dataframe Tersendiri Sebagai Data Input"""

X = df[['user', 'book']].values
X

"""#### Mapping Data Book-Rating Menjadi Satu Dataframe Tersendiri Sebagai Label"""

y = df['Book-Rating'].values
y

"""#### Scaling Data Menggunakan MinMaxScaler"""

scaler = MinMaxScaler()
y = scaler.fit_transform(y.reshape(-1, 1))
y

"""#### Membagi Data Train dan Data Validasi Dengan Rasio 80:20"""

train_samples = int(0.8 * df.shape[0])
X_train, X_test, y_train, y_test = X[:train_samples], X[train_samples:], y[:train_samples], y[train_samples:]

"""# Modeling and Result

#### Membuat Model Recommender
"""

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_user, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_user = num_user
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = tf.keras.layers.Embedding(
        num_user,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.user_bias = tf.keras.layers.Embedding(num_user, 1)
    self.book_embedding = tf.keras.layers.Embedding(
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.book_bias = tf.keras.layers.Embedding(num_book, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    book_vector = self.book_embedding(inputs[:, 1])
    book_bias = self.book_bias(inputs[:, 1])

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x)

"""#### Menginstansiasi Model"""

model = RecommenderNet(num_user = len(users_to_encoded), num_book = len(isbn_to_encoded), embedding_size = 50)

"""#### Menentukan Optimizer, Loss Function, dan Metric"""

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""#### Melakukan Training Model"""

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 32,
    epochs = 10,
    validation_data = (X_test, y_test)
)

"""#### Prediksi"""

df_book = books
df_rating = ratings

user_id = df_rating['User-ID'].sample(1).iloc[0]
book_get_rating_by_user = df_rating[df_rating['User-ID'] == user_id]

book_not_get_rating = df_book[~df_book['ISBN'].isin(book_get_rating_by_user['ISBN'].values)]['ISBN']
book_not_get_rating = list(
    set(book_not_get_rating)
    .intersection(set(isbn_to_encoded.keys()))
)

book_not_get_rating = [[isbn_to_encoded.get(x)] for x in book_not_get_rating]
user_encoder = users_to_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_get_rating), book_not_get_rating))

ratings_predict = model.predict(user_book_array).flatten()

top_ratings_indices = ratings_predict.argsort()[-10:][::-1]
recommended_book_ids = [
    encoded_to_isbn.get(book_not_get_rating[x][0]) for x in top_ratings_indices
]

print('User-ID:', user_id)
print('Recommended Book ISBN:')
recommended_book = df_book[df_book['ISBN'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
  print(row.ISBN, ':', row._2)

"""# Evaluasi

#### Visualisasi Metrik MSE
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()